{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "from pytz import timezone, UTC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geocode_address(address, api_key):\n",
    "    \"\"\"Geocode an address using the Google Maps Geocoding API.\"\"\"\n",
    "    base_url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "    params = {\"address\": address, \"key\": api_key}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        data = response.json()\n",
    "        \n",
    "        if data['status'] == 'OK':\n",
    "            location = data['results'][0]['geometry']['location']\n",
    "            return location['lat'], location['lng']\n",
    "        else:\n",
    "            print(f\"Geocoding error for address '{address}': {data['status']}\")\n",
    "            return None, None\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request Error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate the distance between two geographic coordinates.\"\"\"\n",
    "    return geodesic((lat1, lon1), (lat2, lon2)).kilometers\n",
    "\n",
    "def preprocess_time_columns(row, header, time_columns):\n",
    "    \"\"\"Preprocess time data in specified columns.\"\"\"\n",
    "    for time_column in time_columns:\n",
    "        if isinstance(time_column, int) and time_column < len(row):\n",
    "            time_str = row[time_column].strip('\"')\n",
    "            time_obj = datetime.fromisoformat(time_str).astimezone(UTC)\n",
    "            row[time_column] = time_obj.isoformat()\n",
    "        elif isinstance(time_column, str) and time_column in header:\n",
    "            column_index = header.index(time_column)\n",
    "            time_str = row[column_index].strip('\"')\n",
    "            time_obj = datetime.fromisoformat(time_str).astimezone(UTC)\n",
    "            row[column_index] = time_obj.isoformat()\n",
    "\n",
    "def preprocess_csv(input_file, output_file, time_columns, api_key, exclude_postcodes, add_distance_column=True):\n",
    "    with open(input_file, 'r', newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        \n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "        \n",
    "        # Write header\n",
    "        header = next(reader)\n",
    "        if add_distance_column and all(col in header for col in ('LAST_DELIVERY_POST_CODE', 'FIRST_COLLECTION_POST_CODE')):\n",
    "            header.append('DELIVERY_DISTANCE')\n",
    "        writer.writerow(header)\n",
    "        \n",
    "        # Store processed rows to check for duplicates\n",
    "        processed_rows = set()\n",
    "        \n",
    "        # Process rows\n",
    "        for row in reader:\n",
    "            # Check if the row is a duplicate\n",
    "            if tuple(row) in processed_rows:\n",
    "                continue  # Skip duplicate rows\n",
    "            processed_rows.add(tuple(row))\n",
    "            \n",
    "            # Preprocess time columns\n",
    "            preprocess_time_columns(row, header, time_columns)\n",
    "            \n",
    "            # Geocode post codes\n",
    "            if all(col in header for col in ('LAST_DELIVERY_POST_CODE', 'FIRST_COLLECTION_POST_CODE')):\n",
    "                last_post_code_index = header.index('LAST_DELIVERY_POST_CODE')\n",
    "                first_post_code_index = header.index('FIRST_COLLECTION_POST_CODE')\n",
    "                last_delivery_lat_index = header.index('LAST_DELIVERY_LATITUDE')\n",
    "                last_delivery_lon_index = header.index('LAST_DELIVERY_LONGITUDE')\n",
    "                first_collection_lat_index = header.index('FIRST_COLLECTION_LATITUDE')\n",
    "                first_collection_lon_index = header.index('FIRST_COLLECTION_LONGITUDE')\n",
    "                \n",
    "                last_post_code = row[last_post_code_index]\n",
    "                first_post_code = row[first_post_code_index]\n",
    "\n",
    "                if last_post_code not in exclude_postcodes:\n",
    "                    lat, lon = geocode_address(last_post_code, api_key)\n",
    "                    if lat is not None and lon is not None:\n",
    "                        row[last_delivery_lat_index] = f\"{lat:.15f}\"\n",
    "                        row[last_delivery_lon_index] = f\"{lon:.15f}\"\n",
    "                \n",
    "                if first_post_code not in exclude_postcodes:\n",
    "                    lat, lon = geocode_address(first_post_code, api_key)\n",
    "                    if lat is not None and lon is not None:\n",
    "                        row[first_collection_lat_index] = f\"{lat:.15f}\"\n",
    "                        row[first_collection_lon_index] = f\"{lon:.15f}\"\n",
    "                \n",
    "                if add_distance_column and all(row[index] for index in (first_collection_lat_index, first_collection_lon_index,\n",
    "                                                                      last_delivery_lat_index, last_delivery_lon_index)):\n",
    "                    distance = calculate_distance(\n",
    "                        float(row[first_collection_lat_index]),\n",
    "                        float(row[first_collection_lon_index]),\n",
    "                        float(row[last_delivery_lat_index]),\n",
    "                        float(row[last_delivery_lon_index])\n",
    "                    )\n",
    "                    row.append(f\"{distance:.2f}\")\n",
    "                elif add_distance_column:\n",
    "                    row.append(\"\")\n",
    "                \n",
    "                time.sleep(0.1)  # To respect API rate limits\n",
    "            \n",
    "            # Write the processed row to the output file\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"AIzaSyAM_Tvk02b6O3lsM1i2gJpXEAR5rEeohyo\"\n",
    "exclude_postcodes = ['EH54 8QX','DOR','NAVAN','WS8 7TS','G75 0QH','M28 5LA','G68 0DA','MK4 4BX','DL3 0XZ','ML5 4RW','NP6 3EA','DD2 3PS','CLONDIKIN','S40 2EB','G5 8JB','DA1 1BU','B24','ME6 5PX','ST4 3PR','SS17 9LD','HU70 0YW','HU70','HD61']\n",
    "\n",
    "files_to_preprocess = [\n",
    "    {'input_file': 'Shipment_bookings.csv', 'output_file': 'Shipment_bookings_processed.csv', 'time_columns': ['FIRST_COLLECTION_SCHEDULE_EARLIEST', 'FIRST_COLLECTION_SCHEDULE_LATEST', 'LAST_DELIVERY_SCHEDULE_EARLIEST', 'LAST_DELIVERY_SCHEDULE_LATEST'], 'add_distance_column': True},\n",
    "    {'input_file': 'GPS_data.csv', 'output_file': 'GPS_data_processed.csv', 'time_columns': ['RECORD_TIMESTAMP'], 'add_distance_column': False},\n",
    "    {'input_file': 'New_bookings.csv', 'output_file': 'New_bookings_processed.csv', 'time_columns': ['FIRST_COLLECTION_SCHEDULE_EARLIEST', 'FIRST_COLLECTION_SCHEDULE_LATEST', 'LAST_DELIVERY_SCHEDULE_EARLIEST', 'LAST_DELIVERY_SCHEDULE_LATEST'], 'add_distance_column': True}\n",
    "]\n",
    "\n",
    "# Preprocess each CSV file\n",
    "for file_info in files_to_preprocess:\n",
    "    preprocess_csv(file_info['input_file'], file_info['output_file'], file_info['time_columns'], api_key, exclude_postcodes, file_info['add_distance_column'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed CSV files into DataFrames\n",
    "shipment_df = pd.read_csv('Shipment_bookings_processed.csv')\n",
    "gps_df = pd.read_csv('GPS_data_processed.csv')\n",
    "\n",
    "# Merge the DataFrames on the \"SHIPMENT_NUMBER\" column\n",
    "merged_df = pd.merge(shipment_df, gps_df, on='SHIPMENT_NUMBER', how='inner')\n",
    "\n",
    "# Rename the columns\n",
    "merged_df.rename(columns={\"PROJECT_ID\": \"SHIPPER_ID\", \"VEHICLE_SIZE\": \"VEHICLE_TYPE\"}, inplace=True)\n",
    "\n",
    "# Save the merged file to a new CSV file\n",
    "merged_df.to_csv(\"Historical_data.csv\",index=False)\n",
    "\n",
    "# Print the number of rows in the merged DataFrame\n",
    "print(f\"Number of rows in merged data: {len(merged_df)}\")\n",
    "\n",
    "# Function to compare latitudes and longitudes with a tolerance\n",
    "def compare_with_tolerance(df, lat_col1, lon_col1, lat_col2, lon_col2, tolerance=1e-3):\n",
    "\n",
    "    return (\n",
    "        np.abs(df[lat_col1] - df[lat_col2]) <= tolerance\n",
    "    ) & (\n",
    "        np.abs(df[lon_col1] - df[lon_col2]) <= tolerance\n",
    "    )\n",
    "\n",
    "# Filter rows where LAST_DELIVERY_LATITUDE matches LAT and LAST_DELIVERY_LONGITUDE matches LON within a tolerance\n",
    "actual_deliveries = merged_df[compare_with_tolerance(\n",
    "    merged_df, \"LAST_DELIVERY_LATITUDE\", \"LAST_DELIVERY_LONGITUDE\", \"LAT\", \"LON\")]\n",
    "\n",
    "# Convert the time columns to datetime\n",
    "actual_deliveries['RECORD_TIMESTAMP'] = pd.to_datetime(actual_deliveries['RECORD_TIMESTAMP'], utc=True)\n",
    "actual_deliveries['LAST_DELIVERY_SCHEDULE_LATEST'] = pd.to_datetime(actual_deliveries['LAST_DELIVERY_SCHEDULE_LATEST'],utc=True)\n",
    "\n",
    "# Determine if the delivery is late\n",
    "actual_deliveries['LATE_DELIVERY'] = (actual_deliveries['RECORD_TIMESTAMP'] >= \n",
    "                                      actual_deliveries['LAST_DELIVERY_SCHEDULE_LATEST'] + timedelta(minutes=30)).astype(int)\n",
    "\n",
    "# Save the filtered rows to a new CSV file\n",
    "actual_deliveries.to_csv(\"Historical_data.csv\",index=False)\n",
    "\n",
    "# Filter the shipment data for the specified date range\n",
    "start_date = \"2023-10-01\"\n",
    "end_date = \"2023-12-31\"\n",
    "filtered_shipments = actual_deliveries[(actual_deliveries['LAST_DELIVERY_SCHEDULE_EARLIEST'] >= start_date) & \n",
    "                                   (actual_deliveries['LAST_DELIVERY_SCHEDULE_LATEST'] <= end_date)]\n",
    "\n",
    "# Save the filtered shipments as a CSV file with the name \"Q4_2023_shipments.csv\"\n",
    "filtered_shipments.to_csv(\"Q4_2023_shipments.csv\", index=False)\n",
    "\n",
    "# Save the late deliveries to a new CSV file\n",
    "late_deliveries = filtered_shipments[filtered_shipments['LATE_DELIVERY'] == 1]\n",
    "late_deliveries.to_csv(\"Late_deliveries.csv\", index=False)\n",
    "\n",
    "# Calculate the total number of shipments\n",
    "\n",
    "# Calculate the percentage of late deliveries\n",
    "total_shipments = len(filtered_shipments)\n",
    "late_count = len(late_deliveries)\n",
    "percentage_late_deliveries = (late_count / total_shipments) * 100\n",
    "\n",
    "print(f\"Total number of shipments: {total_shipments}\")\n",
    "print(f\"Number of late deliveries: {late_count}\")\n",
    "print(f\"Percentage of late deliveries: {percentage_late_deliveries:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load dataframes\n",
    "historical_data = pd.read_csv(\"Historical_data.csv\")\n",
    "new_bookings = pd.read_csv(\"New_bookings_processed.csv\")\n",
    "\n",
    "# Rename the 'VEHICLE_SIZE' column in new_bookings\n",
    "new_bookings.rename(columns={\"VEHICLE_SIZE\": \"VEHICLE_TYPE\"}, inplace=True)\n",
    "\n",
    "# Define the columns to check for null or empty values\n",
    "columns_to_check = ['VEHICLE_TYPE', 'VEHICLE_BUILD_UP', 'FIRST_COLLECTION_POST_CODE', 'LAST_DELIVERY_POST_CODE']\n",
    "\n",
    "# Remove rows with null or empty values in the specified columns\n",
    "historical_data = historical_data.dropna(subset=columns_to_check)\n",
    "new_bookings = new_bookings.dropna(subset=columns_to_check)\n",
    "\n",
    "# Save the updated DataFrames\n",
    "historical_data.to_csv(\"Historical_data.csv\", index=False)\n",
    "new_bookings.to_csv(\"New_bookings_processed.csv\", index=False)\n",
    "\n",
    "# One-hot encode the 'VEHICLE_TYPE' and 'VEHICLE_BUILD_UP' columns\n",
    "historical_data = pd.get_dummies(historical_data, columns=['VEHICLE_TYPE', 'VEHICLE_BUILD_UP'])\n",
    "new_bookings = pd.get_dummies(new_bookings, columns=['VEHICLE_TYPE', 'VEHICLE_BUILD_UP'])\n",
    "\n",
    "# Save the updated DataFrames \n",
    "historical_data.to_csv(\"Historical_data_encoded.csv\", index=False)\n",
    "new_bookings.to_csv(\"New_bookings_processed_encoded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for feature selection\n",
    "def feature_selector(training_data, prediction_data, target_variable):\n",
    "    # Step 1: Find common columns excluding the target variable\n",
    "    common_columns = list(set(training_data.columns) & set(prediction_data.columns) - {target_variable})\n",
    "    \n",
    "    # Step 2: Check the data type of the common columns in the training data\n",
    "    column_types = training_data[common_columns].dtypes\n",
    "    \n",
    "    # Step 3: Keep only columns of type 'bool' and 'DELIVERY_DISTANCE' of type 'float64'\n",
    "    cols_to_use = [col for col in common_columns if column_types[col] == 'bool' or (col == 'DELIVERY_DISTANCE')]\n",
    "    \n",
    "    return cols_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the historical and new datasets\n",
    "historical_data = pd.read_csv('Historical_data_encoded.csv')\n",
    "new_data = pd.read_csv('New_bookings_processed_encoded.csv')\n",
    "\n",
    "selected_features = feature_selector(historical_data,new_data,'LATE_DELIVERY')\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data\n",
    "X = historical_data[selected_features]  # Features\n",
    "y = historical_data['LATE_DELIVERY']  # Target variable\n",
    "\n",
    "# Split into Train and Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the Models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=10000,class_weight='balanced'),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'Neural Network': MLPClassifier()\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[name] = model\n",
    "\n",
    "# Evaluate the Models with Cross-Validation\n",
    "results = {}\n",
    "for name, model in trained_models.items():\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5)  # 5-fold cross-validation\n",
    "    accuracy = cv_scores.mean()\n",
    "    report = classification_report(y_test, model.predict(X_test))\n",
    "    results[name] = {'Cross-Validation Accuracy': accuracy, 'Classification Report': report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of Model Performance\n",
    "for name, result in results.items():\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Cross-Validation Accuracy: {result['Cross-Validation Accuracy']}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(result['Classification Report'])\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Visualization\n",
    "plt.bar(results.keys(), [result['Cross-Validation Accuracy'] for result in results.values()], color='#E97132')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.title('Model Performance')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Features\n",
    "X_new = new_data[selected_features]\n",
    "\n",
    "# Predict using the trained 'Gradient Boosting model\n",
    "y_pred_new = trained_models['Gradient Boosting'].predict(X_new)\n",
    "\n",
    "# Add the predicted labels to the new dataset\n",
    "new_data['Predicted_LATE_DELIVERY'] = y_pred_new\n",
    "\n",
    "# Save the new dataset with predicted labels\n",
    "new_data.to_csv('New_bookings_with_predictions.csv', index=False)\n",
    "\n",
    "# Count the number of rows where the predicted label is late (1)\n",
    "late_rows = new_data[new_data['Predicted_LATE_DELIVERY'] == 1]\n",
    "\n",
    "# Calculate the percentage of late rows\n",
    "percentage_late = (len(late_rows) / len(new_data)) * 100\n",
    "\n",
    "print(f\"The percentage of late shipments in New_bookings.csv is: {percentage_late:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
